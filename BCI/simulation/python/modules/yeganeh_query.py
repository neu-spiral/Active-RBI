import numpy as np
import random

eps = np.power(.1, 10)


class randomQuery(object):
    """ Given the final posterior selects the N most likely options to be queried.
        Attr:
            alp(list[str]): list of symbols used by the framework. Can be switched with
                location of images or one hot encoded images.
        Functions:
            reset():
                this selection method is memoryless, therefore this is void\
            update_query():
                updates the querying method and picks the particular query
        """

    def __init__(self, query, len_query=4):
        self.query = query
        self.len_query = len_query

    def reset(self):
        tmp = None

    def update_query(self, p, p_label=None):
        """ with the final belief over the system, updates the querying method and
            generates len_query most likely queries.
            Args:
                p(list[float]): list of probability distribution over the state estimates
                len_query(int): number of queries in the scheduled query
            Return:
                query(list[str]): queries """
        tmp = [i for i in self.query]
        query = random.sample(tmp, self.len_query)

        return query


class NBestQuery(object):
    """ Given the final posterior selects the N most likely options to be queried.
        Attr:
            query(list[str]): list of symbols used by the framework. Can be switched with
                location of images or one hot encoded images.
        Functions:
            reset():
                this selection method is memoryless, therefore this is void\
            update_query():
                updates the querying method and picks the particular query
        """

    def __init__(self, query, len_query=4):
        self.query = query
        self.len_query = len_query

    def reset(self):
        tmp = None

    def update_query(self, p):
        """ with the final belief over the system, updates the querying method and
            generates len_query most likely queries.
            Args:
                p(list[float]): list of probability distribution over the state estimates
                len_query(int): number of queries in the scheduled query
            Return:
                query(list[str]): queries """
        tmp = [i for i in self.query]
        query = best_selection(tmp, p, self.len_query)

        return query


class MomentumQuerying(object):
    """ picks arms in multi armed bandit(MAB) with the momentum reward.
        reward is convex combination of the momentum of a particular query and the
        entropy. given the rewards makes greedy arm selection.
        Attr:
            query(list[str]): list of symbols used by the framework. Can be switched with
                location of images or one hot encoded images.
            lam(float): in [0,1] convex combination parameter
            gam(float): in [0,1] history decay rate for momentum
            momentum(ndarray[float]): momentum values for each element in the query
            prob_history(list[list[float]]): probability values for each step in the
                history for each element in the query
            last_query(list[str]): final query generated by the system
        Functions:
            reset():
                resets the system memory

                """

    def __init__(self, query, len_query=4, gam=.9, lam=.9):
        self.query = query
        self.lam = lam
        self.gam = gam
        self.len_query = len_query
        self.momentum = np.zeros(len(self.query))
        self.prob_history = []
        self.last_query = []

    def reset(self, lam=.9):
        """ resets the history related items in the query method
            Args:
                lam(float): in [0,1] convex combination parameter """
        self.lam = lam
        self.momentum = np.zeros(len(self.query))
        self.prob_history = []

    def update_query(self, p, p_label=None):
        """ with the final belief over the system, updates the querying method and
            generates len_query most likely queries.
            Args:
                p(list[float]): list of probability distribution over the state estimates
                len_query(int): number of queries in the scheduled query
            Return:
                query(list[str]): queries """
        tmp = p[:]
        self.prob_history.append(tmp)
        self.update_momentum()

        entropy_term = np.array(tmp) * np.log(tmp) + (1.01 - np.array(tmp)) * (
            np.log(1.01 - np.array(tmp)))
        entropy_term[np.isnan(entropy_term)] = 0
        # TODO: Stupid magic number
        reward = (self.lam - 1) * entropy_term + self.lam * self.momentum
        self.update_lam()

        tmp_alp = [i for i in self.query]
        tmp_query = best_selection(tmp_alp, reward, self.len_query)

        self.last_query = [i for i in tmp_query]

        return self.last_query

    def update_momentum(self):
        """ momentum is updated with the particular probability history of the system.
            WARNING!: if called twice without a probability update, will update
            momentum using the same information twice """
        if len(self.prob_history) >= 2:
            # only update momentum for previous terms
            idx_prev_query = [self.query.index(self.last_query[i]) for i in
                              range(len(self.last_query))]
            self.momentum *= self.gam

            for k in idx_prev_query:
                # momentum = current_mass * mass_displacement
                self.momentum[k] += self.prob_history[-1][k] * np.power(10,
                                                                        5) * (
                                            self.prob_history[-1][k] -
                                            self.prob_history[-2][k])

    def update_lam(self):
        """ Handles the handshaking between two objectives. currently just a constant
            shift with number of queries, should be updated logically """
        self.lam = np.max([self.lam - .2, 0])


class MomentumQueryingLog(object):

    def __init__(self, query, len_query=4, gam=1., lam=1., dlam=.9, shift=0,
                 update_lam_flag=True):
        self.query = query
        self.lam = np.float(lam)
        self.dlam = dlam
        self.shift = shift
        self.gam = np.float(gam)
        self.len_query = len_query
        self.update_lam_flag = update_lam_flag
        self.momentum = np.zeros(len(self.query))
        self.prob_history = []
        self.last_query = []

    def reset(self, lam=1):
        """ resets the history related items in the query method
            Args:
                lam(float): in [0,1] convex combination parameter """
        if self.updateLam:
            self.lam = np.float(lam)

        self.momentum = np.zeros(len(self.query))
        self.prob_history = []

    def update_query(self, p, p_label=None):
        """ with the final belief over the system, updates the querying method and
            generates len_query most likely queries.
            Args:
                p(list[float]): list of probability distribution over the state estimates
                len_query(int): number of queries in the scheduled query
            Return:
                query(list[str]): queries """
        tmp = p[:]
        self.prob_history.append(tmp)
        self.update_momentum()
        numSeq = len(self.prob_history)

        entropy_term = np.array(tmp) * np.log(tmp + eps) + (
                1.01 - np.array(tmp)) * (np.log(1.01 - np.array(tmp)))
        entropy_term[np.isnan(entropy_term)] = 0

        if numSeq > 1:
            reward = (self.lam - 1) * entropy_term + (
                    self.lam / numSeq) * self.momentum
        else:
            reward = -entropy_term

        if self.update_lam_flag:
            self.update_lam(len(self.prob_history))

        tmp_alp = [i for i in self.query]
        tmp_query = best_selection(tmp_alp, reward, self.len_query)

        self.last_query = [i for i in tmp_query]

        return self.last_query

    def update_momentum(self):
        """ momentum is updated with the particular probability history of the system.
            WARNING!: if called twice without a probability update, will update
            momentum using the same information twice """
        if len(self.prob_history) >= 2:
            # only update momentum for previous terms
            idx_prev_query = [self.query.index(self.last_query[i]) for i in
                              range(len(self.last_query))]
            self.momentum *= self.gam

            for k in idx_prev_query:
                # momentum = current_mass * mass_displacement
                new_mom = self.prob_history[-1][k] * (
                        np.log(self.prob_history[-1][k] + eps) - np.log(
                    self.prob_history[-2][k] + eps))
                self.momentum[k] += new_mom

    def update_lam(self, leng=1):
        """ Handles the handshaking between two objectives. currently just a constant
            shift with number of queries, should be updated logically """
        thr = 1
        if leng < 10:
            self.lam = np.max(
                [self.lam - self.dlam * ((leng - self.shift) / thr), 0])
        else:
            self.lam = 0

""" Wilson, Aaron, Alan Fern, and Prasad Tadepalli. 
"A bayesian approach for policy learning from trajectory preference queries." 
Advances in neural information processing systems. 2012"""

# [Tong and Koller, 2000] S. Tong and D. Koller. Support vector machine active
# learning with applications to text classification. In ICML, 2000
class TongQuery(object):

    def __init__(self, query, len_query=4):
        self.query = query
        self.len_query = len_query
        self.prob_history = []
        self.last_query = []
        self.I_q = 1e-5 * np.eye(len(query) - 1)

    def reset(self, lam=1):
        """ resets the history related items in the query method
            Args:
                lam(float): in [0,1] convex combination parameter """
        self.momentum = np.zeros(len(self.query))
        self.prob_history = []

    def update_query(self, p, p_label=None):
        """ with the final belief over the system, updates the querying method and
            generates len_query most likely queries.
            Args:
                p(list[float]): list of probability distribution over the state estimates
                len_query(int): number of queries in the scheduled query
            Return:
                query(list[str]): queries """
        tmp = p[:]
        self.prob_history.append(tmp)
        numSeq = len(self.prob_history)

        reward = -tmp * np.log(tmp)

        tmp_alp = [i for i in self.query]
        tmp_query = best_selection(tmp_alp, reward, self.len_query)

        self.last_query = [i for i in tmp_query]

        return self.last_query


""" Zhang, Jianming, Shugao Ma, and Stan Sclaroff. 
    "MEEM: robust tracking via multiple experts using entropy minimization." 
    European conference on computer vision. Springer, Cham, 2014."""

class FIRQueryJamshid(object):

    def __init__(self, query, len_query=4):
        self.query = query
        self.len_query = len_query
        self.prob_history = []
        self.last_query = []
        self.I_q = 1e-5 * np.eye(len(query) - 1)

    def reset(self, lam=1):
        """ resets the history related items in the query method
            Args:
                lam(float): in [0,1] convex combination parameter """
        self.momentum = np.zeros(len(self.query))
        self.prob_history = []

    def update_query(self, p, p_label=None):
        """ with the final belief over the system, updates the querying method and
            generates len_query most likely queries.
            Args:
                p(list[float]): list of probability distribution over the state estimates
                len_query(int): number of queries in the scheduled query
            Return:
                query(list[str]): queries """
        tmp = p[:]
        self.prob_history.append(tmp)
        numSeq = len(self.prob_history)

        # B = np.diag(tmp) - np.matmul(np.expand_dims(tmp, 1),
        #                              np.transpose(np.expand_dims(tmp, 1)))
        # reward = np.diagonal(B)

        reward = -1 / (-np.log2(tmp) * tmp)

        tmp_alp = [i for i in self.query]
        tmp_query = best_selection(tmp_alp, reward, self.len_query)

        self.last_query = [i for i in tmp_query]

        return self.last_query


""" Dasgupta, Sanjoy, and Daniel Hsu. 
    "Hierarchical sampling for active learning." 
    Proceedings of the 25th international conference on Machine learning.
    ACM, 2008."""

class SanjoyQuery(object):

    def __init__(self, query, len_query=4):
        self.query = query
        self.len_query = len_query
        self.prob_history = []
        self.last_query = []
        self.I_q = 1e-5 * np.eye(len(query) - 1)

    def reset(self, lam=1):
        """ resets the history related items in the query method
            Args:
                lam(float): in [0,1] convex combination parameter """
        self.momentum = np.zeros(len(self.query))
        self.prob_history = []

    def update_query(self, p, p_label=None):
        """ with the final belief over the system, updates the querying method and
            generates len_query most likely queries.
            Args:
                p(list[float]): list of probability distribution over the state estimates
                len_query(int): number of queries in the scheduled query
            Return:
                query(list[str]): queries """
        tmp = p[:]
        self.prob_history.append(tmp)
        numSeq = len(self.prob_history)

        # B = np.diag(tmp) - np.matmul(np.expand_dims(tmp, 1),
        #                              np.transpose(np.expand_dims(tmp, 1)))
        # reward = np.diagonal(B)

        num_random_q = int(self.len_query / 2)

        reward = -1 / (-np.log2(tmp) * tmp)
        tmp_alp = [i for i in self.query]
        tmp_query1 = best_selection(tmp_alp, reward,
                                    self.len_query - num_random_q)

        tmp_rem_alp = list(set(tmp_alp) - set(tmp_query1))
        tmp_query2 = random.sample(tmp_rem_alp, num_random_q)

        self.last_query = [i for i in tmp_query1 + tmp_query2]

        return self.last_query


class ReyniQuery(object):
    """ Selects query using Renyi entropy
        Attr:
            alp(list[str]): list of symbols used by the framework. Can be switched with
                location of images or one hot encoded images.
            lam(float): in [0,1] convex combination parameter
            gam(float): in [0,1] history decay rate for momentum
            momentum(ndarray[float]): momentum values for each element in the alp
            prob_history(list[list[float]]): probability values for each step in the
                history for each element in the alp
            last_query(list[str]): final query generated by the system
        Functions:
            reset():
                resets the system memory """

    def __init__(self, query, alph=1, len_query=4):
        self.query = query
        self.alph = alph
        self.len_query = len_query
        self.last_query = []

    def reset(self, lam=.9):

        tmp = None

    def update_query(self, p, p_label=None):
        """ with the final belief over the system, updates the querying method and
            generates len_query most likely queries.
            Args:
                p(list[float]): list of probability distribution over the state estimates
                len_query(int): number of queries in the scheduled query
            Return:
                query(list[str]): queries """

        tmp = p[:]

        if self.alph == 1:
            shannon_entropy = np.array(tmp) * np.log(tmp) + (
                    1.01 - np.array(tmp)) * (
                                  np.log(1.01 - np.array(tmp)))
            shannon_entropy[np.isnan(shannon_entropy)] = 0
            renyi_entropy = -shannon_entropy
        elif self.alph == 0:
            renyi_entropy = np.ones(len(self.query)) * np.log(2)
        elif self.alph == np.inf:
            renyi_entropy = -np.ones(len(self.query)) * np.log(
                np.max(tmp) + eps)
        else:
            bin_norm = [p ** self.alph + (1 - p) ** self.alph for p in tmp]
            renyi_entropy = 1 / (1 - self.alph) * np.log(bin_norm)
            renyi_entropy[np.isnan(renyi_entropy)] = 0

        reward = renyi_entropy

        tmp_alp = [i for i in self.query]
        tmp_query = best_selection(tmp_alp, reward, self.len_query)

        self.last_query = [i for i in tmp_query]

        return self.last_query


class RenyiMomentumQuery(object):

    def __init__(self, query, alph=1, len_query=4, gam=1., lam=1., dlam=.9,
                 dalpha=.1, seq_lam=10, shift=0, update_lam_flag=True,
                 update_alpha_flag=False):
        self.query = query
        self.alph = alph
        self.lam = np.float(lam)
        self.init_lam = np.float(lam)
        self.init_alpha = np.float(alph)
        self.dlam = dlam
        self.dalpha = dalpha
        self.seq_lam = seq_lam
        self.shift = shift
        self.gam = np.float(gam)
        self.len_query = len_query
        self.update_lam_flag = update_lam_flag
        self.update_alpha_flag = update_alpha_flag
        self.Renyimomentum = np.zeros(len(self.query))
        self.prob_history = []
        self.last_query = []

    def reset(self):
        """ resets the history related items in the query method
            Args:
                lam(float): in [0,1] convex combination parameter """
        if self.update_lam_flag:
            self.lam = np.float(self.init_lam)
            self.alph = np.float(self.init_alpha)

        self.Renyimomentum = np.zeros(len(self.query))
        self.prob_history = []

    def update_query(self, p, p_label):
        """ with the final belief over the system, updates the querying method and
            generates len_query most likely queries.
            Args:
                p(list[float]): list of probability distribution over the state estimates
                len_query(int): number of queries in the scheduled query
            Return:
                query(list[str]): queries """
        tmp = p[:]
        self.prob_history.append(tmp)
        self.update_Renyimomentum()
        numSeq = len(self.prob_history)

        renyi_entropy = np.zeros(len(self.query))

        if any(p_label):
            if self.alph == 1:
                for q in self.query:
                    q_indx = self.query.index(q)
                    nonZero_indx = list(np.where(p_label[1, :, q_indx] != 0)[0])
                    zero_indx = list(np.where(p_label[1, :, q_indx] == 0)[0])
                    p_log_pos = np.sum(
                        [tmp[ind] * p_label[1, ind, q_indx] * np.log(
                            (tmp[ind] / (
                                    np.sum(tmp[nonZero_indx]) + eps)) + eps) for
                         ind in
                         nonZero_indx])
                    p_log_neg = np.sum(
                        [tmp[ind] * p_label[0, ind, q_indx] * np.log(
                            (tmp[ind] / (np.sum(tmp[zero_indx]) + eps)) + eps)
                         for ind
                         in zero_indx])

                    renyi_entropy[q_indx] = (p_log_pos + p_log_neg)

            elif self.alph == 0:
                renyi_entropy = np.ones(len(self.query)) * np.log(2)

            elif self.alph == np.inf:
                tmp_max = np.max(tmp)
                for q in self.query:
                    q_indx = self.query.index(q)
                    nonZero_indx = list(np.where(p_label[1, :, q_indx] != 0)[0])
                    zero_indx = list(np.where(p_label[1, :, q_indx] == 0)[0])
                    p_log_pos = np.log(np.max(
                        [(tmp[ind] / (np.sum(tmp[nonZero_indx]) + eps)) for ind
                         in nonZero_indx]))
                    p_log_neg = np.log(np.max(
                        [(tmp[ind] / (np.sum(tmp[zero_indx]) + eps)) for ind
                         in zero_indx]))
                    # P(l=1 | v, phi)
                    p_label_pos = np.sum(
                        [p_log_pos * p_label[1, p, q_indx] * tmp[p] for p in
                         range(len(tmp))])
                    # P(l=0 | v, phi)
                    p_label_neg = np.sum(
                        [p_log_neg * p_label[0, p, q_indx] * tmp[p] for p
                         in range(len(tmp))])

                    renyi_entropy[q_indx] = 1 / (
                            1 - self.alph) * p_label_pos + p_label_neg

            else:
                for q in self.query:
                    q_indx = self.query.index(q)
                    nonZero_indx = list(np.where(p_label[1, :, q_indx] != 0)[0])
                    zero_indx = list(np.where(p_label[1, :, q_indx] == 0)[0])
                    p_log_pos = np.log(np.sum(
                        [(tmp[ind] / (
                                np.sum(tmp[nonZero_indx]) + eps)) ** self.alph
                         for ind in nonZero_indx]) + eps)
                    p_log_neg = np.log(np.sum(
                        [(tmp[ind] / (
                                np.sum(tmp[zero_indx]) + eps)) ** self.alph
                         for ind in zero_indx]) + eps)
                    # P(l=1 | v, phi)
                    p_label_pos = np.sum(
                        p_log_pos * p_label[1, :, q_indx] * tmp)
                    # P(l=0 | v, phi)
                    p_label_neg = np.sum(
                        p_log_neg * p_label[0, :, q_indx] * tmp)
                    renyi_entropy[q_indx] = -1 / (1 - self.alph) * (
                            p_label_pos + p_label_neg)

        else:
            if self.alph == 1:
                shannon_entropy = np.array(tmp) * np.log(tmp + eps) + (
                        1.01 - np.array(tmp)) * (
                                      np.log(1 - np.array(tmp) + eps))
                shannon_entropy[np.isnan(shannon_entropy)] = 0
                renyi_entropy = -shannon_entropy
            elif self.alph == 0:
                renyi_entropy = -np.ones(len(self.query)) * np.log(2)
            elif self.alph == np.inf:
                renyi_entropy = np.ones(len(self.query)) * np.log(
                    np.max(tmp) + eps)
            # else:
            #     bin_norm = [p * np.log(p ** self.alph + eps) + (1 - p) * np.log(
            #         1 - p ** self.alph + eps) for p in tmp]
            #     renyi_entropy = -1 / (1 - self.alph) * np.array(bin_norm)
            # elif self.alph > 1:
            #     bin_norm = [p * np.log(p ** self.alph + eps) + (1 - p) * np.log(
            #         1 - p ** self.alph + eps) for p in tmp]
            #     renyi_entropy = 1 / (1 - self.alph) * np.array(bin_norm)
            #
            # else:
            #     bin_norm = [p * np.log(p ** self.alph + eps) + (1 - p) * np.log(
            #         1 - p ** self.alph + eps) for p in tmp]
            #     renyi_entropy = -1 / (1 - self.alph) * np.array(bin_norm)
            #     # bin_norm = [p * np.log(p ** self.alph + eps) + (1 - p) * np.log(
            #     # 1 - p ** self.alph + eps) for p in tmp]
            #     # renyi_entropy = -1 / (1 - self.alph) * np.array(bin_norm)
            else:
                tmp_aziz = [np.power(tmp, self.alph)]
                tmp_aziz = np.log(np.sum(tmp_aziz) - tmp_aziz + eps)
                tmp_aziz_2 = [np.log((1 - x) ** self.alph + eps)
                              for x in tmp]
                renyi_entropy = \
                    (1 / (1 - self.alph) * (1. - tmp) * (
                            tmp_aziz_2 - tmp_aziz))[0]

        renyi_entropy[np.isnan(renyi_entropy)] = 0

        if numSeq > 1:
            reward = (1 - self.lam) * renyi_entropy + (
                    self.lam / numSeq) * self.Renyimomentum
        else:
            reward = renyi_entropy

        if self.update_lam_flag:
            self.update_lam(len(self.prob_history))

        if self.update_alpha_flag:
            self.update_alpha(len(self.prob_history))

        tmp_alp = [i for i in self.query]
        tmp_query = best_selection(tmp_alp, reward, self.len_query)

        self.last_query = [i for i in tmp_query]

        return self.last_query

    def update_Renyimomentum(self):
        """ momentum is updated with the particular probability history of the system.
            WARNING!: if called twice without a probability update, will update
            momentum using the same information twice """
        if len(self.prob_history) >= 2:
            prob_1 = self.prob_history[-1]
            prob_0 = self.prob_history[-2]
            # only update momentum for previous terms
            idx_prev_query = [self.query.index(self.last_query[i]) for i in
                              range(len(self.last_query))]
            self.Renyimomentum *= self.gam
            # self.Renyimomentum *= self.gam

            new_mom = np.zeros(len(self.Renyimomentum))
            # momentum = current_mass * mass_displacement
            if self.alph == 1:
                new_mom = prob_1 * (np.log(prob_1 + eps) - np.log(prob_0 + eps))
            elif self.alph == 0:
                new_mom = - np.log(prob_0 + eps)
            elif self.alph == np.inf:
                max_ind = np.argmax(prob_1 / (prob_0 + eps))
                new_mom[max_ind] = np.log(np.max(prob_1 / (prob_0 + eps)))
            else:
                bin_norm = prob_1 ** self.alph / (
                        prob_0 ** (self.alph - 1) + eps)
                new_mom = 1 / (self.alph - 1) * np.log(bin_norm + eps)

            self.Renyimomentum = new_mom

    def update_lam(self, leng=1):
        """ Handles the handshaking between two objectives. currently just a constant
            shift with number of queries, should be updated logically """
        thr = 1
        if leng < self.seq_lam:
            self.lam = np.max(
                [self.lam - self.dlam * ((leng - self.shift) / thr), 0])
        else:
            self.lam = 0

    def update_alpha(self, leng=1):
        """
         """
        # if leng < 5:
        #     self.alph = self.alph + self.dalpha / 2
        # elif leng < 20:
        #     self.alph = self.alph + self.dalpha * 5
        if leng > 5:
            self.alph = self.alph + self.dalpha
            # print(self.alph)


def best_selection(list_el, val, len_query):
    """ given set of elements and a value function over the set, picks the len_query
        number of elements with the best value.
        Args:
            list_el(list[str]): the set of elements
            val(list[float]): values for the corresponding elements
            len_query(int): number of elements to be picked from the set
        Return:
            query(list[str]): elements from list_el with the best value """
    max_p_val = np.sort(val)[::-1]
    max_p_val = max_p_val[0:len_query]

    query = []
    for idx in range(len_query):
        try:
            idx_q = np.where(val == max_p_val[idx])[0][0]
        except:
            import pdb
            pdb.set_trace()

        q = list_el[idx_q]
        val = np.delete(val, idx_q)
        list_el.remove(q)
        query.append(q)

    return query

#
